{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining \n",
    "\n",
    "## Tutorial 3 - MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 1, 2018**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To understand the generaitve adversarial networks(GANs).\n",
    "* To become familiar with algorithm of training GANs.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Go to File->Open. Drag and drop \"9-GAN.ipynb\" file to the home interface and click upload. \n",
    "* Read the code and complete the exercises.\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top. \n",
    "\n",
    "Lecturers: Chang Xu\n",
    "\n",
    "Tutors: Dalu Guo, Jiayan Qiu, Chaoyue Wang, Xinyuan Chen, Zeyu Feng and Sanjeev Sharma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks are a set of models that basically learn to create synthetic data that is similar to input data it's given. In more formal terms, a GAN is a generative model that learns the probability distribution (or data distribution) of the training examples it is given. From this distribution, we can then create sample outputs. In this class of tutorial, we’ll be creating a GAN that learns to generate synthetic, yet readable, images of MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of these networks is that you have 2 models, a generative model and a discriminative model.\n",
    "![Flowchart](gan_arch.png)\n",
    "\n",
    "A GAN consists of two neural networks, $\\mathcal{D}$ and $\\mathcal{G}$. The network $\\mathcal{G}$ is called a generator, and the network $D$ is called a discriminator. In the simplest case, our data consists of a set $\\mathcal{D}$ of unlabeled data points (for example, images). The goal of the generator is to take random noise as an input and produce an output that \"looks real,\" as if it came from $\\mathcal{D}$. The goal of the discriminator is to take an input and decide whether it came from a generator network or a real data set. We train these networks together, with the hope (often misplaced) that each network will force the other to improve, with the end result that the generator learns to generate highly realistic outputs that consistently \"fool\" the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Task\n",
    "we’re going to create a GAN that will generate MNIST digits that can fool even the best classifiers (and humans too of course). Here’s what we’re going to need:\n",
    "- Real MNIST training images\n",
    "- A generator network that takes in a random noise vector and produces a synthetic image\n",
    "- A discriminator network (a CNN) that learns to distinguish between real and synthetic images. You can think of it as just a binary classifier (1 for real image, 0 for fake)\n",
    "- An optimization procedure that jointly updates both networks through SGD. This is the tricky part as we need to train the generator network to fool the discriminator network, which means that we have unique gradient flows and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The MNIST Dataset\n",
    "Now, we have to import our MNIST images. To do this, we’ll call a TF function called read_data_sets. This loads in the 55,000 training examples in the MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-dacd6a5972ff>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The mnist variable we created above actually contains both the images and their labels. Let's just isolate the images for now. There will be 55,000 images and each of them will be of sixe 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = mnist.train.images[:55000,:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the dataset\n",
    "Let's look at what a random image might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f7dae5ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomNum = random.randint(0,55000)\n",
    "image = x_train[randomNum].reshape([28,28])\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define a CNN classifier function that takes in an image (of size 28 x 28 x 1) as input. The output will be a single scalar number activation that describes whether or not the input image is real or not. In order to do that, let's first define some functions that will help us with creating CNNs in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see how we’re going to compose this network. We’ll start off by passing the image through a convolutional layer. First, we create our weight and bias variables through tf.get_variable. Our first weight matrix (or filter) will be of size 5x5 and will have a output depth of 8. It will be randomly initialized from a normal distribution.\n",
    "\n",
    "Then, we’ll call the function tf.nn.conv2d() through our a helper function called conv2d. tf.nn.conv2d() is the Tensorflow’s function for a common convolution. It takes in 4 arguments. The first is the input volume (our 28 x 28 x 1 image in this case). The next argument is the filter/weight matrix. Finally, you can also change the stride and padding of the convolution. Those two values affect the dimensions of the output volume.\n",
    "\n",
    "As with any convolutional neural network, this module is repeated, and then followed by a series of fully connected layers. At the end of the network, we do a final matrix multiply and return the activation value. For those of you comfortable with CNNs, this is just a simple binary classifier. Nothing fancy.\n",
    "\n",
    "This architecture for this network is based on Tensorflow's sample CNN classifier model\n",
    "[https://www.tensorflow.org/tutorials/mnist/pros/](https://www.tensorflow.org/tutorials/mnist/pros/):\n",
    "1. Convolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "2. Pooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "3. Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "4. Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "5. Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "6. Dense Layer #2 (Logits Layer): 1 neurons to predict the probability if the input image is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x_image, reuse=False):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        #First Conv and Pool Layers\n",
    "        W_conv1 = tf.get_variable('d_wconv1', [5, 5, 1, 8], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('d_bconv1', [8], initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "        #Second Conv and Pool Layers\n",
    "        W_conv2 = tf.get_variable('d_wconv2', [5, 5, 8, 16], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('d_bconv2', [16], initializer=tf.constant_initializer(0))\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "        #First Fully Connected Layer\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7 * 7 * 16, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        #Second Fully Connected Layer\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [32, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        #Final Layer\n",
    "        y_conv=(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "Now that we have our discriminator defined, let’s take a look at the generator module. For this, we’ll be basing our model off the generator introduced in the DCGAN paper. The generator seeks to take a d-dimensional noise vector and upsample it to become a 28 x 28 image. This upsampling is done through a convolutional transpose (or deconvolution) layer. ReLUs and Batch Norm are then used to stabilize the outputs of each layer. \n",
    "\n",
    "The structure of the generator is very similar to that of the discriminator, except we're calling the convolution transpose method, instead of the conv2d one.\n",
    "\n",
    "The conv transpose + relu + batch norm pipeline is repeated 4 times so that the output volume grows larger and larger until a 28 x 28 x 1 image is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim, reuse=False):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        g_dim = 64 #Number of filters of first layer of generator \n",
    "        c_dim = 1 #Color dimension of output (MNIST is grayscale, so c_dim = 1 for us)\n",
    "        s = 28 #Output size of the image\n",
    "        s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16) #We want to slowly upscale the image, so these values will help\n",
    "                                                                  #make that change gradual.\n",
    "\n",
    "        h0 = tf.reshape(z, [batch_size, s16+1, s16+1, 25])\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        #Dimensions of h0 = batch_size x 2 x 2 x 25\n",
    "\n",
    "        #First DeConv Layer\n",
    "        output1_shape = [batch_size, s8, s8, g_dim*4]\n",
    "        W_conv1 = tf.get_variable('g_wconv1', [5, 5, output1_shape[-1], int(h0.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv1 = tf.get_variable('g_bconv1', [output1_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv1 = tf.nn.conv2d_transpose(h0, W_conv1, output_shape=output1_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv1\n",
    "        H_conv1 = tf.contrib.layers.batch_norm(inputs = H_conv1, center=True, scale=True, is_training=True, scope=\"g_bn1\")\n",
    "        H_conv1 = tf.nn.relu(H_conv1)\n",
    "        #Dimensions of H_conv1 = batch_size x 3 x 3 x 256\n",
    "\n",
    "        #Second DeConv Layer\n",
    "        output2_shape = [batch_size, s4 - 1, s4 - 1, g_dim*2]\n",
    "        W_conv2 = tf.get_variable('g_wconv2', [5, 5, output2_shape[-1], int(H_conv1.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv2 = tf.get_variable('g_bconv2', [output2_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv2 = tf.nn.conv2d_transpose(H_conv1, W_conv2, output_shape=output2_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv2\n",
    "        H_conv2 = tf.contrib.layers.batch_norm(inputs = H_conv2, center=True, scale=True, is_training=True, scope=\"g_bn2\")\n",
    "        H_conv2 = tf.nn.relu(H_conv2)\n",
    "        #Dimensions of H_conv2 = batch_size x 6 x 6 x 128\n",
    "\n",
    "        #Third DeConv Layer\n",
    "        output3_shape = [batch_size, s2 - 2, s2 - 2, g_dim*1]\n",
    "        W_conv3 = tf.get_variable('g_wconv3', [5, 5, output3_shape[-1], int(H_conv2.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv3 = tf.get_variable('g_bconv3', [output3_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv3 = tf.nn.conv2d_transpose(H_conv2, W_conv3, output_shape=output3_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='SAME') + b_conv3\n",
    "        H_conv3 = tf.contrib.layers.batch_norm(inputs = H_conv3, center=True, scale=True, is_training=True, scope=\"g_bn3\")\n",
    "        H_conv3 = tf.nn.relu(H_conv3)\n",
    "        #Dimensions of H_conv3 = batch_size x 12 x 12 x 64\n",
    "\n",
    "        #Fourth DeConv Layer\n",
    "        output4_shape = [batch_size, s, s, c_dim]\n",
    "        W_conv4 = tf.get_variable('g_wconv4', [5, 5, output4_shape[-1], int(H_conv3.get_shape()[-1])], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b_conv4 = tf.get_variable('g_bconv4', [output4_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "        H_conv4 = tf.nn.conv2d_transpose(H_conv3, W_conv4, output_shape=output4_shape, \n",
    "                                         strides=[1, 2, 2, 1], padding='VALID') + b_conv4\n",
    "        H_conv4 = tf.nn.tanh(H_conv4)\n",
    "        #Dimensions of H_conv4 = batch_size x 28 x 28 x 1\n",
    "\n",
    "    return H_conv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Sample Image\n",
    "\n",
    "We’ve defined both the generator and discriminator functions. Let’s see what a sample output from an untrained generator looks like. With Tensorflow, we need to first define a session and then create a placeholder for the input to our generator. The purpose of a placeholder is basically to tell Tensorflow \"We're going to input in our random z vector later, but for now, we're going to define this placeholder variable instead\". It lets Tensorflow know about the size of the inputs beforehand. The shape of the placeholder will be None x z_dimensions. The None keyword means that the value can be determined at session runtime. We normally have None as our first dimension so that we can have variable batch sizes (With a batch size of 16, the input to the generator would be 16 x 100). With the None keywoard, we don't have to specify batch_size until later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "z_dimensions = 100\n",
    "z_test_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a variable (sample_image) that holds the output of the generator, and also initialize the random noise vector that we’ll use as input. The np.random.normal function has three arguments. The first and second define the range of the output distribution we want (between -1 and 1 in our case), and the third defines the the shape of the vector (1 x 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_image = generator(z_test_placeholder, 1, z_dimensions)\n",
    "test_z = np.random.normal(-1, 1, [1,z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we initialize all the variables, feed our test_z into the placeholder, and run the session. The sess.run function has two arguments. The first is called the \"fetches\" argument. It defines the value for you're interested in computing. For example, in our case, we want to see what the output of the generator is. If you look back at the last code snippet, the output of the generator function is stored in sample_image. Therefore, we'll use sample_image for our first argument. The second argument is where we input our feed_dict. This data structure is where we provide inputs to all of our placeholders. In our example, we need to feed our test_z variable into the z placeholder we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "temp = (sess.run(sample_image, feed_dict={z_test_placeholder: test_z}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view the output through matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f7dae5cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very convincing MNIST digit right? Let’s look at how we can make our generator better. Enter loss functions and optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "tf.reset_default_graph() #Since we changed our batch size (from 1 to 16), we need to reset our Tensorflow graph\n",
    "\n",
    "sess = tf.Session()\n",
    "x_placeholder = tf.placeholder(\"float\", shape = [None,28,28,1]) #Placeholder for input images to the discriminator\n",
    "z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions]) #Placeholder for input noise vectors to the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the trickiest parts about understanding GANs is that the loss function is a little bit more complex than that of a traditional CNN classifiers (For those, a simple MSE or Hinge Loss would do the trick). If you think back to the introduction, a GAN can be thought of as a zero sum minimax game. The generator is constantly improving to produce more and more realistic images, while the discriminator is trying to get better and better at distinguishing between real and generated images. This means that we need to formulate loss functions that affect both networks. Let’s take a look at the inputs and outputs of our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dx = discriminator(x_placeholder) #Dx will hold discriminator prediction probabilities for the real MNIST images\n",
    "Gz = generator(z_placeholder, batch_size, z_dimensions) #Gz holds the generated images\n",
    "Dg = discriminator(Gz, reuse=True) #Dg will hold discriminator prediction probabilities for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s first think about what we want out of our networks. We want the generator network to create images that will fool the discriminator. The generator wants the discriminator to output a 1 (positive example). Therefore, we want to compute the loss between the Dg and label of 1. This can be done through the tf.nn.sigmoid_cross_entropy_with_logits function. This means that the cross entropy loss will be taken between the two arguments. The \"with_logits\" component means that the function will operate on unscaled values. Basically, this means that instead of using a softmax function to squish the output activations to probability values from 0 to 1, we simply return the unscaled value of the matrix multiplication. Take a look at the last line of our discriminator. There's no softmax or sigmoid layer at the end.\n",
    "The reduce mean function just takes the mean value of all of the components in the matrixx returned by the cross entropy function. This is just a way of reducing the loss to a single scalar value, instead of a vector or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg))) \n",
    "# ensure forward compatibility: function needs to have logits and labels args explicitly used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s think about the discriminator’s point of view. Its goal is to just get the correct labels (output 1 for each MNIST digit and 0 for the generated ones). We’d like to compute the loss between Dx and the correct label of 1 as well as the loss between Dg and the correct label of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once we have our 2 loss functions (d_loss and g_loss), we need to define our optimizers. Keep in mind that the optimizer for the generator network needs to only update the generator’s weights, not those of the discriminator. In order to make this distinction, we need to create 2 lists, one with the discriminator’s weights and one with the generator’s weights. This is where naming all of your Tensorflow variables can come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our two optimizers. In today’s era of deep learning, Adam seems to be the best SGD optimizer as it utilizes adaptive learning rates and momentum. We call Adam's minimize function and also specify the variables that we want it to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = tf.train.AdamOptimizer(learning_rate=0.0002)\n",
    "trainerD = adam.minimize(d_loss, var_list=d_vars)\n",
    "trainerG = adam.minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the best part of neural networks, the algorithm of training loop. During every iteration, there will be two updates being made, one to the discriminator and one to the generator. For the generator update, we’ll feed in a random z vector to the generator and pass that output to the discriminator to obtain a probability score (this is the Dg variable we specified earlier). As we remember from our loss function, the cross entropy loss gets minimized, and only the generator’s weights and biases get updated.\n",
    "\n",
    "We'll do the same for the discriminator update. We’ll be taking a batch of images from the mnist variable we created way at the beginning of our program. These will serve as the positive examples, while the images in the previous section are the negative ones. (It might take several minutes to run. Set 'iteration' to be smaller if you hope to see a primary results with less time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization complete! Time lapses 0.2604432344436646 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "iterations = 3000\n",
    "for i in range(iterations):\n",
    "    z_batch = np.random.normal(-1, 1, size=[batch_size, z_dimensions])\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    real_image_batch = np.reshape(real_image_batch[0],[batch_size,28,28,1])\n",
    "    _,dLoss = sess.run([trainerD, d_loss],feed_dict={z_placeholder:z_batch,x_placeholder:real_image_batch}) #Update the discriminator\n",
    "    _,gLoss = sess.run([trainerG,g_loss],feed_dict={z_placeholder:z_batch}) #Update the generator\n",
    "print(\"optimization complete! Time lapses %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what a sample image looks like after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fa1beab748>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f78d670748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = generator(z_placeholder, 1, z_dimensions,reuse=True)\n",
    "z_batch = np.random.normal(-1, 1, size=[1, z_dimensions])\n",
    "temp = (sess.run(sample_image, feed_dict={z_placeholder: z_batch}))\n",
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Difficulties\n",
    "\n",
    "One note that I’d like to make is that GANs are notoriously difficult to train. Without the right hyperparameters, network architecture, and training procedure, there is a high chance that either the generator or discriminator will overpower the other. A common case of this is the situation where the generator is able to find a flaw in the discriminator by repeatedly outputting an image that fits the data distribution the discriminator is looking for, but is nowhere close to being a readable MNIST digit. The generator has collapsed onto a single point, and therefore we won’t output a variety of digits. There are also cases where the discriminator becomes too powerful and is able to easily make the distinction between real and fake images.\n",
    "\n",
    "The mathematical intuition behind this phenomenon lies in that GANs are typically trained using gradient descent techniques that are designed to find the minimum value of a cost function, rather than to find the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge. Further research into game theory and stable optimization techniques may result in GANs that are as easy to train as ConvNets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts\n",
    " With applications in video frame prediction, text-image mappings, and more, GANs are definitely the hottest topic in deep learning. Hopefully, with this tutorial, you’ve gained a better understanding of how these networks work in practice and how you can build your own with Tensorflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
