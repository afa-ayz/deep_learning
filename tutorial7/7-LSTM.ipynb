{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5329 - Deep Learning \n",
    "\n",
    "## Tutorial 7 - LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 1, 2018**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* How to implement LSTM and GRU in tensorflow\n",
    "* How to use LSTM, GRU, RNN cell in tensorflow\n",
    "* How to process sequece data by deep learning\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Learning to count letter by using RNN, LSTM and GRU\n",
    "\n",
    "Lecturers: Dalu Guo, Jiayan Qiu, Chaoyue Wang, Xinyuan Chen, Zheyu Feng and Sanjeev Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data instruction **\n",
    "* A-Z, a-z, 0-9\n",
    "* The differece number between the upper letters and lower letters with some noise.\n",
    "* 80000 line data, 64000 train data, 16000 validate data\n",
    "\n",
    "** Example **\n",
    "* aAA304     -1\n",
    "* bbB234BbB   0\n",
    "* ccccccC     5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_line(input_char):\n",
    "    num1 = random.randint(1, 30)\n",
    "    num2 = random.randint(1, 30)\n",
    "    src = [chr(input_char) for _ in range(num1)] # lowercase \n",
    "    target = [chr(input_char - 32) for _ in range(num2)] # uppercase\n",
    "    src.extend(target)\n",
    "    \n",
    "    noise_num = random.randint(0, 100)\n",
    "    for _ in range(noise_num):\n",
    "        src.append(str(random.randint(0, 9))) # noise number\n",
    "    random.shuffle(src)    \n",
    "    \n",
    "    return ''.join(src), num1 - num2 + 29\n",
    "    \n",
    "\n",
    "def generate_data(size, filename):\n",
    "    f = open(filename, \"w\")\n",
    "    s = set()\n",
    "    count = 0\n",
    "    while count < size:\n",
    "        c = random.randint(ord('a'), ord('z'))\n",
    "        src, target = generate_line(c)\n",
    "        if src in s or src[::-1] in s:\n",
    "            continue\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print (\"generate %d line\" % count)\n",
    "        s.add(src)\n",
    "        f.write('\\t'.join([src, str(target)]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    generate_data(80000, \"seq.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "* Create train and val data split\n",
    "* Zero padding data and map letter to number \n",
    "* Embedding input to vector\n",
    "* Feed the RNN/LSTM/GRU with embedding sequence \n",
    "* Predict the label from the last state of sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import seq_label\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "MAX_ITERATIONS = 100000\n",
    "VAL_INTERVAL = 1000\n",
    "MODEL_SAVE_INTERVAL = 3000\n",
    "PRINT_INTERVAL = 100\n",
    "batch_size = 64\n",
    "\n",
    "def read_dataset(file_name):\n",
    "    f = open(file_name)\n",
    "    ls = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip() \n",
    "        l = line.split('\\t')\n",
    "        ls.append([l[0], int(l[1])])\n",
    "    \n",
    "    random.shuffle(ls)\n",
    "    return ls[:64000], ls[64000:]\n",
    "\n",
    "#map letter to number \n",
    "#a-z -> 1 - 26\n",
    "#A-Z -> 27 - 52\n",
    "#0-9 -> 53 - 62\n",
    "def create_maps():\n",
    "    dic = {}\n",
    "    counter = 1\n",
    "    for i in range(ord('a'), ord('z') + 1):\n",
    "        dic[chr(i)] = counter\n",
    "        counter += 1\n",
    "    \n",
    "    for i in range(ord('A'), ord('Z') + 1):\n",
    "        dic[chr(i)] = counter\n",
    "        counter += 1\n",
    "        \n",
    "    for i in range(ord('0'), ord('9') + 1):\n",
    "        dic[chr(i)] = counter\n",
    "        counter += 1\n",
    "    \n",
    "    return dic\n",
    "        \n",
    "def create_model(is_train, session, model_path, func_type):\n",
    "    model = seq_label.SeqLabel(func_type, is_train)\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    \n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        #restore model\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        #creat new model\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model\n",
    "\n",
    "def create_batch(datas, maps):\n",
    "    size = len(datas)\n",
    "    seqs = np.zeros((size, seq_label.input_length), dtype = np.int32)\n",
    "    labels = np.zeros(size, dtype = np.int32)\n",
    "    for i in range(size):\n",
    "        labels[i] = datas[i][1]\n",
    "        seq = datas[i][0]\n",
    "        l = seq_label.input_length - len(seq) # zero padding\n",
    "        for j in range(len(seq)):\n",
    "            seqs[i][l + j] = maps[seq[j]] \n",
    "        \n",
    "    return seqs, labels\n",
    "\n",
    "def train_model(maps, func_type):\n",
    "    folder = \"model_\" + func_type\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "    checkpoint_path = os.path.join(folder, \"seq.ckpt\")\n",
    "    train_data, val_data = read_dataset(\"seq.txt\")\n",
    "    pointer = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        model = create_model(True, sess, folder, func_type)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        for step in range(model.global_step.eval() + 1, MAX_ITERATIONS + 1):\n",
    "            start_time = time.time()\n",
    "            if pointer + batch_size >= len(train_data):\n",
    "                random.shuffle(train_data)\n",
    "                pointer = 0\n",
    "            datas = train_data[pointer:pointer + batch_size]\n",
    "            pointer += batch_size               \n",
    "            input_seq, label = create_batch(datas, maps)\n",
    "            step_loss, learning_rate = model.step(sess, \"train\", input_seq, label)\n",
    "            end_time = time.time()\n",
    "            step_time += (end_time - start_time)\n",
    "            loss += step_loss\n",
    "            if step % MODEL_SAVE_INTERVAL == 0:\n",
    "                model.saver.save(sess, checkpoint_path, global_step = step)\n",
    "            if step % PRINT_INTERVAL == 0:\n",
    "                step_time = step_time / PRINT_INTERVAL\n",
    "                loss = loss / PRINT_INTERVAL\n",
    "                print (\"step %d, time %.3f, loss %.3f, rate %.5f\" % (model.global_step.eval(), step_time, loss, learning_rate))\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "            if step % VAL_INTERVAL == 0:\n",
    "                val_rate = val_model(sess, model, val_data, maps)\n",
    "                print (\"val accuracy is %.3f\" % (val_rate))\n",
    "\n",
    "\n",
    "def val_model(sess, model, dataset, maps):\n",
    "    start_pointer = 0\n",
    "    end_pointer = start_pointer + batch_size\n",
    "    total = 0\n",
    "    while start_pointer < len(dataset):\n",
    "        datas = dataset[start_pointer : end_pointer]\n",
    "        start_pointer += batch_size\n",
    "        end_pointer = min(start_pointer + batch_size, len(dataset))               \n",
    "        input_seq, label = create_batch(datas, maps)\n",
    "        answers = model.step(sess, \"test\", input_seq, label)\n",
    "        answer_ids = np.argmax(answers, axis = -1)\n",
    "        total += np.sum(label == answer_ids)\n",
    "    return 1.0 * total / len(dataset)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    maps = create_maps()\n",
    "    if len(sys.argv) < 2:\n",
    "        print (\"python train.py rnn/lstm/gru\")\n",
    "        exit()\n",
    "        \n",
    "    func_type = sys.argv[1]\n",
    "    train_model(maps, func_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Sequene Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import array_ops, embedding_ops\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "\n",
    "gru_size = 50\n",
    "embedding_size = 20\n",
    "input_length = 160\n",
    "vob_size = 52 + 10 + 1\n",
    "output_size = 60\n",
    "gru_keep_prob = 0.5\n",
    "init_lr_rate = 0.001\n",
    "decay_step = 4000\n",
    "decay_rate = 0.5\n",
    "max_gradient_norm = 3\n",
    "\n",
    "class SeqLabel(object):\n",
    "    def __init__(self, func_type, is_train = True, dtype = tf.float32):\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        self.gru_size = gru_size\n",
    "        self.input_seq = tf.placeholder(tf.int32, [None, input_length])\n",
    "        self.label = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        input_embed = self.model_embedding(self.input_seq, vob_size)\n",
    "        _, state = self.model_seq(input_embed, func_type, dtype)\n",
    "        self.predict = self.model_answer(state)\n",
    "        \n",
    "        if is_train:\n",
    "            params = tf.trainable_variables()\n",
    "            self.loss = self.model_loss(self.predict, self.label)\n",
    "            self.learning_rate = tf.train.exponential_decay(init_lr_rate, self.global_step, decay_step, decay_rate, True)\n",
    "            opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, self.norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "            self.updates = opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "        \n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep = 3)\n",
    "    \n",
    "    def step(self, session, func, input_seq, label):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.input_seq] = input_seq\n",
    "        feed_dict[self.label] = label\n",
    "        if func == \"train\":\n",
    "            output_feeds = [self.loss, self.learning_rate, self.updates, self.norm]\n",
    "            outputs = session.run(output_feeds, feed_dict)\n",
    "            return outputs[0], outputs[1]\n",
    "        elif func == \"test\":\n",
    "            output_feeds = [self.predict]\n",
    "            outputs = session.run(output_feeds, feed_dict)\n",
    "            return outputs[0]\n",
    "\n",
    "    def fully_connected(self, inputs, out_size, scope = \"fully_connected\", reuse = False, dtype = tf.float32):\n",
    "        with variable_scope.variable_scope(scope, reuse = reuse, dtype = dtype):\n",
    "            input_depth = inputs.get_shape()[1].value\n",
    "            w = tf.get_variable(\"weight\", shape=[input_depth, out_size])\n",
    "            b = tf.get_variable(\"bias\", shape=[out_size], initializer = init_ops.zeros_initializer())\n",
    "            r = tf.matmul(inputs, w) + b\n",
    "        return r\n",
    "    \n",
    "    #word embedding\n",
    "    def model_embedding(self, seq, vob_size, dtype = tf.float32):\n",
    "        with variable_scope.variable_scope(\"embedding\", dtype = dtype):\n",
    "            init = initializers.xavier_initializer()\n",
    "            word_embedding = tf.get_variable(\"word_embedding\", shape = [vob_size, embedding_size], initializer = init, dtype = dtype)\n",
    "            embeddings = embedding_ops.embedding_lookup(word_embedding, seq)\n",
    "            return embeddings        \n",
    "    \n",
    "    def model_seq(self, seq, func_type, dtype = tf.float32):\n",
    "        seq = tf.transpose(seq, [1, 0, 2])\n",
    "        input_list = tf.unstack(seq, axis = 0)\n",
    "        if func_type == \"rnn\":\n",
    "            outputs, state = self.model_rnn(input_list, dtype)\n",
    "        elif func_type == \"lstm\":\n",
    "            outputs, state = self.model_lstm(input_list, dtype)\n",
    "        elif func_type == \"gru\":\n",
    "            outputs, state = self.model_gru(input_list, dtype)\n",
    "        else:\n",
    "            raise Exception(\"Nothing to do\")\n",
    "         \n",
    "        return outputs, state        \n",
    "    \n",
    "    def model_rnn(self, input_list, dtype = tf.float32):\n",
    "        first_input = input_list[0]\n",
    "        input_shape = first_input.get_shape()\n",
    "        fixed_batch_size = input_shape[0]\n",
    "        if fixed_batch_size.value:\n",
    "            batch_size = fixed_batch_size.value\n",
    "        else:\n",
    "            batch_size = array_ops.shape(first_input)[0]\n",
    "        state = tf.zeros([batch_size, gru_size], dtype)\n",
    "            \n",
    "        outputs = []\n",
    "        for index in range(len(input_list)):\n",
    "            with variable_scope.variable_scope(\"rnn\", dtype = dtype):\n",
    "                reuse = True if index > 0 else False\n",
    "                concat = tf.concat([input_list[index], state], axis = 1)\n",
    "                state = tf.tanh(self.fully_connected(concat, gru_size, reuse = reuse))\n",
    "                outputs.append(state)\n",
    "        return outputs, state \n",
    "    \n",
    "    def model_lstm(self, input_list, dtype = tf.float32):\n",
    "        #fill lstm code here\n",
    "        None\n",
    "    \n",
    "    def model_gru(self, input_list, dtype = tf.float32):\n",
    "        #fill gru code here\n",
    "        None\n",
    "    \n",
    "    def model_tf_lstm(self, input_list, dtype = tf.float32):\n",
    "        #file tensorflow gru, lstm code here\n",
    "        None\n",
    "    \n",
    "    #project last state to answer \n",
    "    def model_answer(self, state, dtype = tf.float32):\n",
    "        with variable_scope.variable_scope(\"model_answer\", dtype = dtype):\n",
    "            predict = self.fully_connected(state, output_size)\n",
    "            return predict\n",
    "        \n",
    "    #softmax with cross entropy \n",
    "    def model_loss(self, predict, labels):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = predict, labels = labels)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "* Implement LSTM, GRU by basic tensorflow function\n",
    "* Try to use tensorflow high level function of LSTM and GRU\n",
    "* Try to adjust parameters in model to achieve better performance\n",
    "* Try to implement attention to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
